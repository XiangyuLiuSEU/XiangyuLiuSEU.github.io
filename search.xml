<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[什么是动态规划]]></title>
    <url>%2F2019%2F04%2F04%2Fwhat-is-dynamic-programming%2F</url>
    <content type="text"><![CDATA[作者：阮行止 链接：https://www.zhihu.com/question/23995189/answer/613096905 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 0. intro 很有意思的问题。以往见过许多教材，对动态规划（DP）的引入属于“奉天承运，皇帝诏曰”式：不给出一点引入，见面即拿出一大堆公式吓人；学生则死啃书本，然后突然顿悟。针对入门者的教材不应该是这样的。恰好我给入门者讲过四次DP入门，迭代出了一套比较靠谱的教学方法，所以今天跑过来献丑。 现在，我们试着自己来一步步“重新发明”DP。 1. 从一个生活问题谈起 先来看看生活中经常遇到的事吧——假设您是个土豪，身上带了足够的1、5、10、20、50、100元面值的钞票。现在您的目标是凑出某个金额w，需要用到尽量少的钞票。 依据生活经验，我们显然可以采取这样的策略：能用100的就尽量用100的，否则尽量用50的……依次类推。在这种策略下，666=6×100+1×50+1×10+1×5+1×1，共使用了10张钞票。 这种策略称为“贪心”：假设我们面对的局面是“需要凑出w”，贪心策略会尽快让w变得更小。能让w少100就尽量让它少100，这样我们接下来面对的局面就是凑出w-100。长期的生活经验表明，贪心策略是正确的。 但是，如果我们换一组钞票的面值，贪心策略就也许不成立了。如果一个奇葩国家的钞票面额分别是1、5、11，那么我们在凑出15的时候，贪心策略会出错： 15=1×11+4×1 （贪心策略使用了5张钞票） 15=3×5 （正确的策略，只用3张钞票） 为什么会这样呢？贪心策略错在了哪里？ 鼠目寸光。 刚刚已经说过，贪心策略的纲领是：“尽量使接下来面对的w更小”。这样，贪心策略在w=15的局面时，会优先使用11来把w降到4；但是在这个问题中，凑出4的代价是很高的，必须使用4×1。如果使用了5，w会降为10，虽然没有4那么小，但是凑出10只需要两张5元。 在这里我们发现，贪心是一种只考虑眼前情况的策略。 那么，现在我们怎样才能避免鼠目寸光呢？ 如果直接暴力枚举凑出w的方案，明显复杂度过高。太多种方法可以凑出w了，枚举它们的时间是不可承受的。我们现在来尝试找一下性质。 重新分析刚刚的例子。w=15时，我们如果取11，接下来就面对w=4的情况；如果取5，则接下来面对w=10的情况。我们发现这些问题都有相同的形式：“给定w，凑出w所用的最少钞票是多少张？”接下来，我们用f(n)来表示“凑出n所需的最少钞票数量”。 那么，如果我们取了11，最后的代价（用掉的钞票总数）是多少呢？ 明显 $\text{cost} = f(4) + 1 = 4 + 1 = 5$，它的意义是：利用11来凑出15，付出的代价等于f(4)加上自己这一张钞票。现在我们暂时不管f(4)怎么求出来。 依次类推，马上可以知道：如果我们用5来凑出15，cost就是 $f(10) + 1 = 2 + 1 = 3$ 。 那么，现在w=15的时候，我们该取那种钞票呢？当然是各种方案中，cost值最低的那一个！ 取11：$\text{cost}=f(4)+1=4+1=5$ 取5： $\text{cost}=f(10)+1=2+1=3$ 取1： $\text{cost}=f(14)+1=4+1=5$ 显而易见，cost值最低的是取5的方案。我们通过上面三个式子，做出了正确的决策！ 这给了我们一个至关重要的启示—— $f(n)$ 只与 $f(n-1), f(n-5), f(n-11)$ 相关；更确切地说：$f(n)=\min{f(n-1),f(n-5),f(n-11)}+1$ 这个式子是非常激动人心的。我们要求出f(n)，只需要求出几个更小的f值；既然如此，我们从小到大把所有的f(i)求出来不就好了？注意一下边界情况即可。代码如下： 我们以 $O(n)$ 的复杂度解决了这个问题。现在回过头来，我们看看它的原理： $f(n)$ 只与 $f(n-1), f(n-5), f(n-11)$ 的值相关。 我们只关心 $f(w)$ 的值，不关心是怎么凑出 w 的。 这两个事实，保证了我们做法的正确性。它比起贪心策略，会分别算出取1、5、11的代价，从而做出一个正确决策，这样就避免掉了“鼠目寸光”！ 它与暴力的区别在哪里？我们的暴力枚举了“使用的硬币”，然而这属于冗余信息。我们要的是答案，根本不关心这个答案是怎么凑出来的。譬如，要求出f(15)，只需要知道f(14),f(10),f(4)的值。**其他信息并不需要。**我们舍弃了冗余信息。我们只记录了对解决问题有帮助的信息——f(n). 我们能这样干，取决于问题的性质：求出f(n)，只需要知道几个更小的f©。我们将求解f©称作求解f(n)的“子问题”。 这就是DP（动态规划，dynamic programming）. 将一个问题拆成几个子问题，分别求解这些子问题，即可推断出大问题的解。 思考题：请稍微修改代码，输出我们凑出w的方案 2. 几个简单的概念 【无后效性】 一旦f(n)确定，“我们如何凑出f(n)”就再也用不着了。 要求出f(15)，只需要知道f(14),f(10),f(4)的值，而f(14),f(10),f(4)是如何算出来的，对之后的问题没有影响。 “未来与过去无关”，这就是无后效性。 （严格定义：如果给定某一阶段的状态，则在这一阶段以后过程的发展不受这阶段以前各段状态的影响。） 【最优子结构】 回顾我们对f(n)的定义：我们记“凑出n所需的最少钞票数量”为f(n). f(n)的定义就已经蕴含了“最优”。利用w=14,10,4的最优解，我们即可算出w=15的最优解。 大问题的最优解可以由小问题的最优解推出，这个性质叫做“最优子结构性质”。 引入这两个概念之后，我们如何判断一个问题能否使用DP解决呢？ 能将大问题拆成几个小问题，且满足无后效性、最优子结构性质。 3. DP的典型应用：DAG最短路 问题很简单：给定一个城市的地图，所有的道路都是单行道，而且不会构成环。每条道路都有过路费，问您从S点到T点花费的最少费用。 ​ 这个问题能用DP解决吗？我们先试着记从S到P的最少费用为f§. 想要到T，要么经过C，要么经过D。从而 $f(T)=\min⁡{f©+20,f(D)+10}$. 好像看起来可以DP。现在我们检验刚刚那两个性质： - 无后效性：对于点P，一旦f§确定，以后就只关心f§的值，不关心怎么去的。 - 最优子结构：对于P，我们当然只关心到P的最小费用，即f§。如果我们从S走到T是 $S \to P\to Q\to T$ ，那肯定S走到Q的最优路径是 $S\to P\to Q$ 。对一条最优的路径而言，从S走到**沿途上所有的点（子问题）**的最优路径，都是这条大路的一部分。这个问题的最优子结构性质是显然的。 既然这两个性质都满足，那么本题可以DP。式子明显为： $f§=\min⁡{f®+w_{R→P}}$ 其中R为有路通到P的所有的点， $w_{R→P}$ 为R到P的过路费。 代码实现也很简单，拓扑排序即可。 4. 对DP原理的一点讨论 【DP的核心思想】 DP为什么会快？ 无论是DP还是暴力，我们的算法都是在可能解空间内，寻找最优解。 来看钞票问题。暴力做法是枚举所有的可能解，这是最大的可能解空间。 DP是枚举有希望成为答案的解。这个空间比暴力的小得多。 也就是说：DP自带剪枝。 DP舍弃了一大堆不可能成为最优解的答案。譬如： 15 = 5+5+5 被考虑了。 15 = 5+5+1+1+1+1+1 从来没有考虑过，因为这不可能成为最优解。 从而我们可以得到DP的核心思想：尽量缩小可能解空间。 在暴力算法中，可能解空间往往是指数级的大小；如果我们采用DP，那么有可能把解空间的大小降到多项式级。 一般来说，解空间越小，寻找解就越快。这样就完成了优化。 【DP的操作过程】 一言以蔽之：大事化小，小事化了。 将一个大问题转化成几个小问题； 求解小问题； 推出大问题的解。 【如何设计DP算法】 下面介绍比较通用的设计DP算法的步骤。 首先，把我们面对的局面表示为x。这一步称为设计状态。 对于状态x，记我们要求出的答案(e.g. 最小费用)为f(x).我们的目标是求出f(T). 找出f(x)与哪些局面有关（记为p），写出一个式子（称为状态转移方程），通过f§来推出f(x). 【DP三连】 设计DP算法，往往可以遵循DP三连： 我是谁？ ——设计状态，表示局面 我从哪里来？ 我要到哪里去？ ——设计转移 设计状态是DP的基础。接下来的设计转移，有两种方式：一种是考虑我从哪里来（本文之前提到的两个例子，都是在考虑“我从哪里来”）；另一种是考虑我到哪里去，这常见于求出f(x)之后，更新能从x走到的一些解。这种DP也是不少的，我们以后会遇到。 总而言之，“我从哪里来”和“我要到哪里去”只需要考虑清楚其中一个，就能设计出状态转移方程，从而写代码求解问题。前者又称pull型的转移，后者又称push型的转移。（这两个词是 @阮止雨妹妹告诉我的，不知道源出处在哪） 思考题：如何把钞票问题的代码改写成“我到哪里去”的形式？ 提示：求出f(x)之后，更新f(x+1),f(x+5),f(x+11). 5. 例题：最长上升子序列 扯了这么多形而上的内容，还是做一道例题吧。 最长上升子序列（LIS）问题：给定长度为n的序列a，从a中抽取出一个子序列，这个子序列需要单调递增。问最长的上升子序列（LIS）的长度。 e.g. 1,5,3,4,6,9,7,8的LIS为1,3,4,6,7,8，长度为6。 如何设计状态（我是谁）？ 我们记 $f(x)$ 为以 $a_x$ 结尾的LIS长度，那么答案就是 $\max{f(x)}$ . 状态x从哪里推过来（我从哪里来）？ 考虑比x小的每一个p：如果 $a_x&gt;a_p$ ，那么f(x)可以取f§+1. 解释：我们把 $a_x$ 接在 $a_p$ 的后面，肯定能构造一个以 $a_x$ 结尾的上升子序列，长度比以 $a_p$ 结尾的LIS大1.那么，我们可以写出状态转移方程了： $f(x)=\max_{p&lt;x , a_p&lt;a_x }⁡{f§}+1$ 至此解决问题。两层for循环，复杂度 $O(n^2)$ . 从这三个例题中可以看出，DP是一种思想，一种“大事化小，小事化了”的思想。带着这种思想，DP将会成为我们解决问题的利器。 最后，我们一起念一遍DP三连吧——我是谁？我从哪里来？我要到哪里去？ 6. 习题 如果读者有兴趣，可以试着完成下面几个习题： 一、请采取一些优化手段，以 $O(n\log n)$ 的复杂度解决LIS问题。 提示：可以参考这篇博客 Junior Dynamic Programming–动态规划初步·各种子序列问题 二、“按顺序递推”和“记忆化搜索”是实现DP的两种方式。请查阅资料，简单描述“记忆化搜索”是什么。并采用记忆化搜索写出钞票问题的代码，然后完成P1541 乌龟棋 - 洛谷 。 三、01背包问题是一种常见的DP模型。请完成P1048 采药 - 洛谷。 作者：王勐 链接：https://www.zhihu.com/question/23995189/answer/35429905 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 动态规划的本质不在于是递推或是递归，也不需要纠结是不是内存换时间。 理解动态规划并不需要数学公式介入，只是完全解释清楚需要点篇幅…首先需要明白哪些问题不是动态规划可以解决的，才能明白为神马需要动态规划。不过好处时顺便也就搞明白了递推贪心搜索和动规之间有什么关系，以及帮助那些总是把动规当成搜索解的同学建立动规的思路。当然熟悉了之后可以直接根据问题的描述得到思路，如果有需要的话再补充吧。 动态规划是对于 某一类问题 的解决方法！！重点在于如何鉴定“某一类问题”是动态规划可解的而不是纠结解决方法是递归还是递推！ 怎么鉴定dp可解的一类问题需要从计算机是怎么工作的说起…计算机的本质是一个状态机，内存里存储的所有数据构成了当前的状态，CPU只能利用当前的状态计算出下一个状态（不要纠结硬盘之类的外部存储，就算考虑他们也只是扩大了状态的存储容量而已，并不能改变下一个状态只能从当前状态计算出来这一条铁律） 当你企图使用计算机解决一个问题是，其实就是在思考如何将这个问题表达成状态（用哪些变量存储哪些数据）以及如何在状态中转移（怎样根据一些变量计算出另一些变量）。所以所谓的空间复杂度就是为了支持你的计算所必需存储的状态最多有多少，所谓时间复杂度就是从初始状态到达最终状态中间需要多少步！ 太抽象了还是举个例子吧： 比如说我想计算第100个非波那契数，每一个非波那契数就是这个问题的一个状态，每求一个新数字只需要之前的两个状态。所以同一个时刻，最多只需要保存两个状态，空间复杂度就是常数；每计算一个新状态所需要的时间也是常数且状态是线性递增的，所以时间复杂度也是线性的。 上面这种状态计算很直接，只需要依照固定的模式从旧状态计算出新状态就行（a[i]=a[i-1]+a[i-2]），不需要考虑是不是需要更多的状态，也不需要选择哪些旧状态来计算新状态。对于这样的解法，我们叫递推。 非波那契那个例子过于简单，以至于让人忽视了阶段的概念，所谓阶段是指随着问题的解决，在同一个时刻可能会得到的不同状态的集合。非波那契数列中，每一步会计算得到一个新数字，所以每个阶段只有一个状态。想象另外一个问题情景，假如把你放在一个围棋棋盘上的某一点，你每一步只能走一格，因为你可以东南西北随便走，所以你当你同样走四步可能会处于很多个不同的位置。从头开始走了几步就是第几个阶段，走了n步可能处于的位置称为一个状态，走了这n步所有可能到达的位置的集合就是这个阶段下所有可能的状态。 现在问题来了，有了阶段之后，计算新状态可能会遇到各种奇葩的情况，针对不同的情况，就需要不同的算法，下面就分情况来说明一下： 假如问题有n个阶段，每个阶段都有多个状态，不同阶段的状态数不必相同，一个阶段的一个状态可以得到下个阶段的所有状态中的几个。那我们要计算出最终阶段的状态数自然要经历之前每个阶段的某些状态。 好消息是，有时候我们并不需要真的计算所有状态，比如这样一个弱智的棋盘问题：从棋盘的左上角到达右下角最短需要几步。答案很显然，用这样一个弱智的问题是为了帮助我们理解阶段和状态。某个阶段确实可以有多个状态，正如这个问题中走n步可以走到很多位置一样。但是同样n步中，有哪些位置可以让我们在第n+1步中走的最远呢？没错，正是第n步中走的最远的位置。换成一句熟悉话叫做“下一步最优是从当前最优得到的”。所以为了计算最终的最优值，只需要存储每一步的最优值即可，解决符合这种性质的问题的算法就叫贪心。如果只看最优状态之间的计算过程是不是和非波那契数列的计算很像？所以计算的方法是递推。 既然问题都是可以划分成阶段和状态的。这样一来我们一下子解决了一大类问题：一个阶段的最优可以由前一个阶段的最优得到。 如果一个阶段的最优无法用前一个阶段的最优得到呢？ 什么你说只需要之前两个阶段就可以得到当前最优？那跟只用之前一个阶段并没有本质区别。最麻烦的情况在于你需要之前所有的情况才行。 再来一个迷宫的例子。在计算从起点到终点的最短路线时，你不能只保存当前阶段的状态，因为题目要求你最短，所以你必须知道之前走过的所有位置。因为即便你当前再的位置不变，之前的路线不同会影响你的之后走的路线。这时你需要保存的是之前每个阶段所经历的那个状态，根据这些信息才能计算出下一个状态！ 每个阶段的状态或许不多，但是每个状态都可以转移到下一阶段的多个状态，所以解的复杂度就是指数的，因此时间复杂度也是指数的。哦哦，刚刚提到的之前的路线会影响到下一步的选择，这个令人不开心的情况就叫做有后效性。 刚刚的情况实在太普遍，解决方法实在太暴力，有没有哪些情况可以避免如此的暴力呢？ 契机就在于后效性。 有一类问题，看似需要之前所有的状态，其实不用。不妨也是拿最长上升子序列的例子来说明为什么他不必需要暴力搜索，进而引出动态规划的思路。 假装我们年幼无知想用搜索去寻找最长上升子序列。怎么搜索呢？需要从头到尾依次枚举是否选择当前的数字，每选定一个数字就要去看看是不是满足“上升”的性质，这里第i个阶段就是去思考是否要选择第i个数，第i个阶段有两个状态，分别是选和不选。哈哈，依稀出现了刚刚迷宫找路的影子！咦慢着，每次当我决定要选择当前数字的时候，只需要和之前选定的一个数字比较就行了！这是和之前迷宫问题的本质不同！这就可以纵容我们不需要记录之前所有的状态啊！既然我们的选择已经不受之前状态的组合的影响了，那时间复杂度自然也不是指数的了啊！虽然我们不在乎某序列之前都是什么元素，但我们还是需要这个序列的长度的。所以我们只需要记录以某个元素结尾的LIS长度就好！因此第i个阶段的最优解只是由前i-1个阶段的最优解得到的，然后就得到了DP方程（感谢 @韩曦 指正） $$ LIS(i)=max{LIS(j)+1} \ \ \ \ j&lt;i \ and\ a[j] &lt; a[i] $$ 所以一个问题是该用递推、贪心、搜索还是动态规划，完全是由这个问题本身阶段间状态的转移方式决定的！ 每个阶段只有一个状态-&gt;递推； 每个阶段的最优状态都是由上一个阶段的最优状态得到的-&gt;贪心； 每个阶段的最优状态是由之前所有阶段的状态的组合得到的-&gt;搜索； 每个阶段的最优状态可以从之前某个阶段的某个或某些状态直接得到而不管之前这个状态是如何得到的-&gt;动态规划。 每个阶段的最优状态可以从之前某个阶段的某个或某些状态直接得到 这个性质叫做最优子结构； 而不管之前这个状态是如何得到的 这个性质叫做无后效性。 另：其实动态规划中的最优状态的说法容易产生误导，以为只需要计算最优状态就好，LIS问题确实如此，转移时只用到了每个阶段“选”的状态。但实际上有的问题往往需要对每个阶段的所有状态都算出一个最优值，然后根据这些最优值再来找最优状态。比如背包问题就需要对前i个包（阶段）容量为j时（状态）计算出最大价值。然后在最后一个阶段中的所有状态种找到最优值。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>dp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PyTorch 的 dataloader 如何读取变长数据]]></title>
    <url>%2F2019%2F04%2F03%2Fpytorch-dataloader-load-different-length-data%2F</url>
    <content type="text"><![CDATA[本文转载自 知乎 Charlie的语音处理实验室 原文链接：https://zhuanlan.zhihu.com/p/60129684 最近在做一个新的声学模型，其中遇到一个点就是每个sentence的长度不一样的花，直接用dataloader的读取是有问题的。查了下中文资料，大家大多数这个问题都是趋于用torch.nn.utils.rnn.PackedSequence来打包的，这个在dataloader里面其实就不太适用，pytorch论坛上提到用dataloader的collate_fn来处理的，所以想写个资料总结下 。 pytorch里面dataset的工作逻辑： pytorch的数据载入主要是这么几个逻辑，从底层一步步来讲，我用h5矩阵，图片和音频三个方面来举例，首先是逻辑层次是，首先把data装进用torch.utils.data.Dataset装进一个dataset的对象里面，然后在把dataset这个对象传递给一个torch.utils.data.DataLoader dataset的工作逻辑 数据集的切分一般在dataset这个对象上做处理，支持随机切分等，详见torch.utils.data - PyTorch master documentation，一般来讲，我都是写一个torch.utils.data.Dataset的子类，里面就三个成员函数，初始化，长度和读取，一般在读取你自己定义的读取方法，我习惯的是h5矩阵的话，就读一段（子矩阵），图片就是一张图，或者一段音频。 这里面有个很关键的点，就是dataset的逻辑是一次读一个item，最好不要在dataset层面一次slice一段，slice这个层面的事情交给dataloader来做，原因我一会说。 记住dataset的逻辑在于装和item读取，预处理，其他都不要做。 dataloader的工作逻辑 dataloader层面主要就是slice读取数据，shuffle也是在这个层面来做。 dataloader有几个关键点，很多地方都零零碎碎的提到过，我总结下， 是稀松平常的batch_size, sampler, shuffle这几个稀松平常的不提，shuffle是在dataset的item层面做混洗， 注意，num_workers是一个多线程的读取，当batchsize&gt;1的时候，多线程读取item, 然后各个item调用一个collate_fn合并成新的tensor，其中h5依然是个坑，anaconda安装的h5是不支持多线程的，请参考并行 HDF5 和 h5py安装并行h5，至于num*_*worker以及pin_memoru的具体使用，参考云梦：Pytorch 提速指南，不重复造轮子。 关于这个collate\fn是重点，当开启多线程了一个，多线程先后读取了dataset里面batch_size个item以后，生成了一个list,里面每个元素就是batchsize个item，然后用collatefn合并，如果没有指定的collatefns的话，就直接合并成一个高一维的tensor。 collatefns的工作逻辑 coolatefns的输入是个list，长度为batchsize，其中各个元素是各个item，函数的目的就是合并。 当各个item变长时，不指定collatefns合并就会报错，懒人方法就是把在dataset里面的读取函数把tensor加到最长，就可以直接merge。 当使用collatefns时，pytorch论坛上有人写了一个函数，我贴过来，大家配合注释看看： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def pad_tensor(vec, pad, dim): """ args: vec - tensor to pad pad - the size to pad to dim - dimension to pad return: a new tensor padded to 'pad' in dimension 'dim' """ pad_size = list(vec.shape) pad_size[dim] = pad - vec.size(dim) return torch.cat([vec, torch.zeros(*pad_size)], dim=dim)class PadCollate: """ a variant of callate_fn that pads according to the longest sequence in a batch of sequences """ def __init__(self, dim=0): """ args: dim - the dimension to be padded (dimension of time in sequences) """ self.dim = dim def pad_collate(self, batch): """ args: batch - list of (tensor, label) reutrn: xs - a tensor of all examples in 'batch' after padding ys - a LongTensor of all labels in batch """ # find longest sequence max_len = max(map(lambda x: x[0].shape[self.dim], batch)) # pad according to max_len batch = map(lambda (x, y): (pad_tensor(x, pad=max_len, dim=self.dim), y), batch) # stack all xs = torch.stack(map(lambda x: x[0], batch), dim=0) ys = torch.LongTensor(map(lambda x: x[1], batch)) return xs, ys def __call__(self, batch): return self.pad_collate(batch) 调用使用： 1train_loader = DataLoader(ds, ..., collate_fn=PadCollate(dim=0)) 来源：DataLoader for various length of data 对于读取了以后的数据，在rnn中的工作逻辑，pytorch的文档也提到过 total_length is useful to implement the packsequence-&gt;recurrentnetwork-&gt;unpacksequence pattern in a Module wrapped in DataParallel. See this FAQ sectionfor details. 来源：torch.nn - PyTorch master documentation 关于读取到了的padding的变长数据，如何pack，请参考 @尹相楠 的： 尹相楠：PyTorch 训练 RNN 时，序列长度不固定怎么办？]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【转载】 腾讯算法实习面经]]></title>
    <url>%2F2019%2F04%2F03%2Frepost-tencent-intern-interview-summary%2F</url>
    <content type="text"><![CDATA[牛客网 腾讯算法实习面试总结—论面试官虐我的一百种方式]]></content>
      <categories>
        <category>求职</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Git 使用笔记]]></title>
    <url>%2F2019%2F04%2F03%2Fgit-note%2F</url>
    <content type="text"><![CDATA[廖雪峰的 Git 教程 少用 Pull 多用 Fetch 和 Merge 见到很多人说过这个经验，原因就是 git pull 把过程的细节都隐藏了起来，大部分时候是没有问题的，但是当代码出错时可能会造成损失。很多时候我们宁愿做的慢一些，也不愿意返工重来 一般的做法是： 12git fetch origin # 下载远程分支的更新git merge origin/master # 合并远程分支到当前分支 如果你想在合并前查看本地分支和远程分支的差异，可以使用下面的命令： 1git diff master origin/master 单独进行下载和合并是一个好的做法，你可以先看看下载的是什么，然后再决定是否和本地代码合并，方便使用。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nvcc 编译报错：找不到 "cuda_runtime.h"]]></title>
    <url>%2F2019%2F04%2F02%2Fnvcc-fatal-error-cuda-runtime-h%2F</url>
    <content type="text"><![CDATA[奇怪的错误 今天在安装 PANet 时遇到了一个奇怪的错误： 12Compiling nms kernels by nvcc...cc1plus: fatal error: cuda_runtime.h: No such file or directory 按理说不应该出现这种奇怪的错误，cuda_runtime.h 就安静地躺在 /usr/local/cuda/include 目录下，cuda 安装是没有问题的，这个仓库在实验室的服务器上也跑过，完全没有问题。然而就是这样的错误费了老半天时间也无法定位原因，网上的解决办法无非就是环境变量的问题，多次确认之后环境变量是没有问题的 😩 难道要因为这个错误重装 cuda？ 重装是不太可能的，服务器上配环境太费事， 于是查看 nvcc 命令帮助，果然其中写着可以用 -I 选项指定包含头文件的路径，迅速在 PANet/lib/make.sh 中有关 nvcc 的命令加上 -I /usr/local/cuda/include ，运行，成功 😎 估计是环境变量出现了某种错误，虽然到最后也没有搞明白为什么会出现这个错误，但好歹是解决了这个问题。这个问题说明了遇事不能只靠百度，要自己好好分析]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>cuda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通过 SSH 隧道连接远程服务器的 Jupyter Notebook]]></title>
    <url>%2F2019%2F02%2F24%2FRemote-jupyter-notebook%2F</url>
    <content type="text"><![CDATA[参考: https://www.howtoing.com/how-to-install-run-connect-to-jupyter-notebook-on-remote-server 在远程服务器上没有安装浏览器的情况下，通过在本地建立 SSH 隧道的方法使用服务器的 Jupyter Notebook 1. 服务器端 首先确保服务器端安装了 Jupyter Notebook，如果需要使用 conda 环境，还要安装 ipykernel 等包 进入虚拟环境，运行 Jupyter Notebook 1jupyter notebook 根据输出的信息可以看到无法找到可用的浏览器 12345[I 09:56:37.551 NotebookApp] Serving notebooks from local directory: /home/liuxiangyu[I 09:56:37.551 NotebookApp] The Jupyter Notebook is running at:[I 09:56:37.552 NotebookApp] http://172.17.0.2:8888/?token=******[I 09:56:37.552 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).[W 09:56:37.557 NotebookApp] No web browser found: could not locate runnable browser. 注意：如果报错信息提示地址已经被占用，那么久需要修改一下 Jupyter Notebook 的默认地址。修改方式如下： 配置 Jupyter Notebook 1jupyter notebook --generate-config 运行命令后将会在主目录下生成 .jupyter/jupyter_notebook_config.py 文件，打开文件找到 #c.NotebookApp.ip = 'localhost'，把 # 号去掉，localhost 改成自己的 ip 地址（在上面输出信息中可以看到） 保存后关闭文件，重启 Jupyter Notebook 即可 至此，服务器端的准备已经完成了，我们需要记住服务器的地址 172.17.0.2 和端口号 8008 2. 本地 以 Windows 系统为例，下载 putty 打开 putty 后在服务器地址和端口处正确填写信息，然后在左侧 ssh 选项下选择 Tunnels Source port 填写本地想用的端口号，以 8000 为例 Destination 填写 服务器地址 172.17.0.2:8888 其他选项不要修改，最后不要忘记点击 Add，然后连接即可 连接到服务器后进入相应的虚拟环境，运行 Jupyter Notebook，然后打开浏览器输入地址 https://localhost:8000 即可打开 notebook. 第一次进入 notebook 可能需要输入 token，按照提示输入相应的 token 即可 Done! 😎]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>env</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Detectron 安装步骤]]></title>
    <url>%2F2019%2F01%2F20%2Fdetectron-installation%2F</url>
    <content type="text"><![CDATA[Requirements： Linux 16.04, Python2, NVIDIA GPU (Detectron 目前只有 GPU 版本) CUDA 9.0, cuDNN7.0.5 Caffe2, COCO API 为保持 Python 环境的独立性与完整性，安装前新建一个新的虚拟环境，以下安装过程在虚拟环境中进行 123# 在主环境下conda create -n detectron python=2.7source activate detectron [TOC] CUDA &amp; cuDNN 安装 CUDA9.0 和 cuDNN7.0.5，具体安装步骤参考英伟达官网 Caffe2 1. 从源码编译 以下为源码编译的过程，尝试过安装 Pre-Built Binaries，结果失败了，也可以直接安装预编译文件（推荐），[2. 安装预编译文件](#2. 安装预编译文件) 安装依赖包 1234567891011121314151617181920212223242526sudo apt-get updatesudo apt-get install -y --no-install-recommends \ build-essential \ git \ libgoogle-glog-dev \ libgtest-dev \ libiomp-dev \ libleveldb-dev \ liblmdb-dev \ libopencv-dev \ libopenmpi-dev \ libsnappy-dev \ libprotobuf-dev \ openmpi-bin \ openmpi-doc \ protobuf-compiler \ python-dev \ python-pip \ libgflags-dev \ cmakepip install --user \ future \ numpy \ protobuf \ typing \ hypothesis pip install 使用默认镜像下载速度较慢，可以选择使用清华大学 pypi 镜像 123456# for temporary usepip install -i https://pypi.tuna.tsinghua.edu.cn/simple some-package# set to defaultpip install pip -U # upgrade pip to the latest version (&gt;=10.0.0)pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple 下载代码仓库并编译 1234git clone https://github.com/pytorch/pytorch.git &amp;&amp; cd pytorchgit submodule update --init --recursive # 安装所需子模块conda install pyyaml # 安装缺少的依赖包python setup.py install 如果编译顺利通过，恭喜，接下来测试 caffe2 安装是否正确，[3. 安装后测试](#3. 安装后测试) 错误信息1：服务器安装的 git 在执行 clone 命令时报错找不到 https 协议，应该是安装不完整导致的，可以在当前环境下重新安装 git 后重试（使用命令 conda install git，推荐）或者使用以下命令代替： 1git clone git://github.com/pytorch/pytorch.git &amp;&amp; cd pytorch 错误信息2（未解决）：在执行安装命令时 (python setup.py install)，出现以下错误： 1······/libmklml_intel.so: file not recognized: File truncated. 该错误指向 pytorch/third_party/ideep/mkl-dnn/external/mklml_lnx_2019.0.1.20180928/lib/libmklml_intel.so 文件，可能是文件不完整导致的错误，但是未找到原因及解决方法。如果出现此错误请尝试通过预编译文件安装 caffe2. 其他错误：查看 Caffe2 troubleshooting 2. 安装预编译文件 Caffe2 只提供 Anaconda 的预编译安装包，需要安装 Anaconda 或 Miniconda 首先添加清华大学维护的 PyTorch 源，下载速度更快： 1conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/ 然后执行安装操作： 1conda install pytorch-nightly 3. 安装后测试 安装完成后，测试安装是否成功 123456# To check if Caffe2 build was successfulpython -c 'from caffe2.python import core' 2&gt;/dev/null &amp;&amp; echo "Success" || echo "Failure"# To check if Caffe2 GPU build was successful# This must print a number &gt; 0 in order to use Detectronpython -c 'from caffe2.python import workspace; print(workspace.NumCudaDevices())' 根据执行结果判断安装是否成功 从源码编译需注意： If the caffe2 Python package is not found, you likely need to adjust your PYTHONPATH environment variable to include its location (/path/to/caffe2/build, where build is the Caffe2 CMake build directory). COCO API 依赖： setuptools&gt;=18.0 cython&gt;=0.27.3 matplotlib&gt;=2.1.0 123456conda install setuptools cython matplotlibCOCOAPI=/path/to/clone/cocoapi # 指定安装路径git clone https://github.com/cocodataset/cocoapi.git $COCOAPIcd $COCOAPI/PythonAPImake#python setup.py install --user Detectron 下载代码仓库： 12DETECTRON=/path/to/clone/detectron # 指定安装路径git clone https://github.com/facebookresearch/detectron $DETECTRON 安装 Python 依赖： 1pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -r $DETECTRON/requirements.txt 然后 make: 1cd $DETECTRON &amp;&amp; make 安装完成后运行测试： 1python $DETECTRON/detectron/tests/test_spatial_narrow_as_op.py 结果输出 OK，安装完毕 输出以下提示信息不必理会： 12No handlers could be found for logger "caffe2.python.net_drawer"net_drawer will not run correctly. Please install the correct dependencies. Troubleshooting 参照 Detectron Troubleshooting Reference https://github.com/facebookresearch/Detectron/blob/master/INSTALL.md https://caffe2.ai/docs/getting-started.html?platform=ubuntu&amp;configuration=prebuilt https://blog.csdn.net/weixin_43624538/article/details/84712617 P.S. CUDA8.0 cuDNN5.1.10 安装失败，原因：PyTorch 需要 cuDNN&gt;=7.0 查看 CUDA 和 cuDNN 版本： 12345# CUDAcat /usr/local/cuda/version.txt# cuDNNcat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2]]></content>
      <categories>
        <category>安装</category>
      </categories>
      <tags>
        <tag>detectron</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YOLOv3 安装步骤]]></title>
    <url>%2F2019%2F01%2F15%2FYOLOv3-installation%2F</url>
    <content type="text"><![CDATA[操作系统：Linux 16.04 依赖： CUDA OpenCV darknet 写在前面：conda 是一款非常好用的 python 环境管理工具，建议安装 Anaconda 或 Miniconda。安装及使用请参阅网上教程，安装完成后记得添加清华大学 tuna 镜像。 Anaconda 镜像使用帮助 CUDA 按照网上教程或英伟达官方网站正确安装 CUDA 和 cuDNN（实验室服务器上的版本是 cuda9.0 cudnn7.1） OpenCv 如果安装了 conda，首先进入你想要安装 YOLO 的虚拟环境。第一次使用 Anaconda 请先创建虚拟环境。 1conda create --name YOURNAME python=3.6 numpy pandas matplotlib （YOURNAME 替换为你想要的名字）上述命令将创建新的 python 虚拟环境，并安装常用工具包 numpy, pandas, matplotlib，创建完成后进入虚拟环境 12source activate YOURNAMEsource deactivate # use this one to exit 进入环境后开始安装 OpenCV 1conda install opencv 安装完毕即可 darknet 首先下载 github 仓库 12git clone https://github.com/AlexeyAB/darknet.gitcd darknet 然后打开 Makefile 文件，修改以下选项： 123GPU = 1CUDNN = 1OPENCV = 1 然后在 darknet 目录下执行 1make 如果没有报错，YOLOv3 就安装成功了。测试一下 1./darknet 可以看到输出为： 1usage: ./darknet &lt;function&gt; Reference https://github.com/AlexeyAB/darknet https://pjreddie.com/darknet/yolo/ 🦄]]></content>
      <categories>
        <category>安装</category>
      </categories>
      <tags>
        <tag>yolo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PANet 安装步骤]]></title>
    <url>%2F2019%2F01%2F15%2FPANet-installation%2F</url>
    <content type="text"><![CDATA[Requirements 操作系统：Linux 16.04 平台：PyTorch 依赖 pytorch = 0.4.0 torchvision &gt;= 0.2.0 cython matplotlib numpy scipy opencv pyyaml packaging pycocotools – for coco dataset tensorboardX – for logging in TensorBoard PyTorch 使用 conda 安装 PyTorch 比较简单，首先进入 conda 环境，然后执行 1conda install pytorch=0.4.0 torchvision cuda90 -c pytorch 其余 cython 等包都可以使用 conda install 或者 pip install 命令来安装，就不再重复了 编译 PANet 首先下载 github 仓库： 12git clone https://github.com/ShuLiu1993/PANet.gitcd PANet 然后编译 12cd lib # please change to this directorysh make.sh 等待编译完成即可。训练代码在 PANet/tools 文件夹下。 如果安装过程报错找不到 cuda_runtime.h，可以查看这里 Reference https://github.com/ShuLiu1993/PANet https://github.com/roytseng-tw/Detectron.pytorch]]></content>
      <categories>
        <category>安装</category>
      </categories>
      <tags>
        <tag>panet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[查找回文字符串：马拉车算法 Manacher's Algorithm]]></title>
    <url>%2F2018%2F12%2F20%2FManacher-Algorithm%2F</url>
    <content type="text"><![CDATA[在 LeetCode 看到一个题，给定一个字符串，返回最大长度的回文子字符串。 Example: 12Input: "babad"Output: "bab" or "aba" 最简单的方法就是暴力循环遍历，但是算法复杂度为 $O(n^3)$, 因此记录一个线性复杂度的算法——马拉车算法。 LeetCode：https://leetcode.com/problems/longest-palindromic-substring 英文解释：https://articles.leetcode.com/longest-palindromic-substring-part-ii 中文解释：https://www.felix021.com/blog/read.php?2040 Manacher’s Algorithm 首先，假设输入字符串为 abaaba，显然输入即为最长的回文字符串，因此输出应为 abaaba。让我们分两步来解决这个问题。 1. 预处理 我们想一下可能会发现，回文字符串分为两种情况：奇数长度和偶数长度。这两种情况显然是无法直接合并处理的，因此马拉车算法首先对输入的字符串进行了一下处理：在每个字符的两侧插入一个特定字符 #，例如： 12S = &apos;abaaba&apos;T = &apos;#a#b#a#a#b#a#&apos; 这样做的好处是所有的字符串都变成了奇数长度，这样我们就不需要分别考虑了 预处理部分的代码如下： 123456def preProcess(self, s): l = [c for c in s] ret = '#'.join(l) # add ^ and $ at both bounds to avoid out of index problem return '^#'+ret+'#$' 2. 算法部分 算法的思想是这样的，对于一个预处理之后的字符串 T，定义一个具有相同长度的数组 P，使得 P[i] 等于以 T[i] 为中心的 T 中最长回文字符串的半径，对于上面的例子 12T = # a # b # a # a # b # a #P = 0 1 0 3 0 1 6 1 0 3 0 1 0 我们可以发现，P 具有某种对称性质，我们可以利用这个性质在一定程度上减小我们的计算复杂度。那么所有的情况都是对称的吗？可惜的是并不是，只有在某种特定的条件下这种情况才成立。但是没关系，我们一样可以利用这个性质减少计算，只是需要找到不满足这种情况的条件就可以了 我们来看一个稍微复杂一点的例子，S = ‘babcbabcbaccba’ 上图中假设当前 i 为 13， 实线表示回文字符串 abcbabcba的中心，虚线表示两侧的边界。我们可以看到，由于回文字符串的对称性质，我们可以快速的知道 P[13] 的值，也就是等于 P[9] 处的值。然后继续看 现在我们到了下标为 15 的位置，p[15] 的值是多少呢？如果我们继续根据对称性质，那么就会得到 P[15] = P[7] = 7，但很显然是错误的。 如果以 T[15] 为中心，我们得到的回文字符串为 #a#b#c#b#a#，而 T[7] 处的回文字符串要更长，这是因为我们当前的以 C 为中心的回文字符串并不能完全包含以 T[7] 为中心的回文字符串，因此就造成了不满足对称性质的情况。解决办法也很简单，只要取 P[i] = min(P[i'], R-i) 就可以了。 用伪代码总结一下： 123if P[i&apos;] &lt;= R - ithen P[i] = P[i&apos;]else P[i] &lt;= P[i&apos;] 然后我们需要确定 R 和 C 的更新策略：如果以 i 为中心的回文字符串的超过了 R，则令新的中心C = i，然后把 R 扩展到新的回文字符串的右边界。 全部代码如下 12345678910111213141516171819202122232425262728293031323334353637383940414243class Solution: def longestPalindrome(self, s): """ :type s: str :rtype: str """ # Manacher's algorithm T = self.preProcess(s) length = len(T) C, R = 0, 0 P = [0] * length i = 1 while i &lt; length-1: # i_mirror can reduce the computation of P[i] i_mirror = 2*C - i # equals to C - (i - C) P[i] = min(P[i_mirror], R-i) if R &gt; i else 0 # expand the palindrome centered at i while T[i + P[i] + 1] == T[i - P[i] - 1]: P[i] += 1 # adjust center if the expanded palindrome pasts R if R &lt; i + P[i]: C = i R = i + P[i] i += 1 maxLength = max(P) centerIndex = P.index(maxLength) start = (centerIndex - maxLength) // 2 stop = start + maxLength return s[start:stop] def preProcess(self, s): l = [c for c in s] ret = '#'.join(l) # add ^ and $ at both bounds to avoid out of index problem return '^#'+ret+'#$' 最后 按理说线性复杂度的算法已经是很快了，当然 LeetCode 上还有很多大神的神仙代码更快，反正我是看不懂…… 🙂 完]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PyTorch 调用 GPU 报 CUDA unknown error]]></title>
    <url>%2F2018%2F12%2F18%2Fpytorch-cuda-unknown-error%2F</url>
    <content type="text"><![CDATA[最近为了跑 PANet，在服务器上安装了 Detectron.pytorch，安装过程还挺顺利，但是只要调用 GPU 运算就报未知错误，网上搜索一番发现可能是显卡驱动安装有问题，导致 Torch 调用显卡时无法正常初始化。这里记录一下网上的解决方法。 报错信息 1RuntimeError: cuda runtime error (30) : unknown error at /opt/conda/conda-bld/pytorch_1524586445097/work/aten/src/THC/THCTensorRandom.cu:25 方法1：重新安装显卡驱动和 CUDA 既然是驱动问题，那么自然地重新安装一下最新版的显卡驱动应该就没问题了，注意驱动安装完成之后最好要重启一下机器。安装完驱动之后使用 conda 重新安装 PyTorch。但是由于服务器是公用资源，为了不影响同学使用，只得使用权宜之计。 方法2：root 权限运行 python 网上的解决方法除了重装驱动之外，还有一种暂时的解决办法。因为正确安装显卡驱动会保证 Torch 调用显卡时自动进行正常的初始化，那我也可以手动赋予 python root 权限去初始化显卡。 在尝试的过程中发现，由于服务器环境比较混乱，sudo 提升权限之后运行的不是我自己的 python，而且 PYTHONPATH 和 一些环境变量也不对。解决方法如下： 首先在 python 代码中加上 12345import osimport syssys.path.append('/path/to/cocoapi/PythonAPI') # my code needs pycocotoolsos.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID" # optionalos.environ["CUDA_VISIBLE_DEVICES"]="0" 然后在运行 1sudo /path/to/your/python tools/train_net_step.py --dataset dota --cfg xx.yml --use_tfboard 然后就可以开始训练了 完]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>cuda</tag>
        <tag>error</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编译时遇到 /tmp 文件夹空间不足的解决办法]]></title>
    <url>%2F2018%2F12%2F11%2FNo-Space-in-tmp%2F</url>
    <content type="text"><![CDATA[今天在服务器上编译 PyTorch 时遇到了 /tmp 文件夹空间不足的问题，一般来说安装 Ubuntu 时给 / 挂载点分配足够的硬盘空间就不会遇到这个问题，但是服务器有很多人用，文件比较混乱，/挂载点已经达到了 100% 的空间使用率，因此百度到了一个解决办法 其实解决方法很简单，只需要在有硬盘空间的挂载点下（例如 /home ）新建一个临时文件夹供编译时临时使用就可以了 新建文件夹 在用户目录下新建临时文件夹，并使之生效即可 123cd /home/lxymkdir tmpexport TMPDIR = /home/lxy/tmp 这样重新执行编译命令就可以顺利编译了 还可以将最后一句代码写进 .bashrc 文件，然后 source 一下，以后临时文件都会存放在该临时文件夹 完]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[将 DOTA 数据集的标注转换为 COCO 格式]]></title>
    <url>%2F2018%2F12%2F10%2FConvert-dataset-to-coco-like%2F</url>
    <content type="text"><![CDATA[DOTA 数据集：http://captain.whu.edu.cn/DOTAweb/index.html COCO 数据集：http://cocodataset.org/#download COCO API：https://github.com/cocodataset/cocoapi API make 报错，安装 Cython 即可 1conda install cython COCO 数据集简介 COCO 数据集包含 instance，keypoint 和 caption 等部分，本文只介绍 instance 相关内容 COCO 数据集的组织方式 coco ├── annos.txt (optional) ├── annotations ├── classes.txt (optional) └── images annotations 文件夹放数据集的标注文件（json格式），images 文件夹放数据集的所有图片，（annos.txt 放数据集的原始标注文件，class.txt 放标注的类别名称，每行一个类别，不含背景） COCO 的数据标注格式 COCO 数据集以 json 文件格式存储数据集的标注信息，标注的格式可以参考 官网 和这个 知乎专栏，在这里就不重复了。 确定了标注的格式以后，分析 DOTA 数据集的标注格式，可以提取其中的信息然后以 json 格式存储下来就可以了 格式转换脚本 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import jsonimport dota_utils as utilimport osfrom PIL import Imageinfo = &#123;"description": "DOTA dataset from WHU", "url": "http://caption.whu.edu.cn", "year": 2018, "version": "1.0"&#125;licenses = &#123;"url": "http://creativecommons.org/licenses/by-nc/2.0/", "id": 1, "name": "Attribution-NonCommercial License"&#125;categories = []for i, catName in enumerate(util.wordname_15, start=1): categories.append(&#123;"id": i, "name": "%s" % catName, "supercategory": "%s" % catName&#125;)images = []annotations = []aug = "/home/lxy/dota/data/aug"augmented = "/home/lxy/dota/data/augmented"train_small = "/home/lxy/dota/data/train_small"trainsplit_HBB = "/home/lxy/dota/data/trainsplit_HBB"val_small = "/home/lxy/dota/data/val_small"valsplit_HBB = "/home/lxy/dota/data/valsplit_HBB"dataset_path = [augmented, train_small, trainsplit_HBB, val_small, valsplit_HBB]imgid = 0annid = 0for path in dataset_path: img_path = os.path.join(path, "images") label_path = os.path.join(path, "labelTxt") for file in os.listdir(label_path): img_name = file.replace("txt", "png") im = Image.open(os.path.join(img_path, img_name)) w, h = im.size imgid += 1 images.append(&#123;"license": 1, "file_name": "%s" % img_name, \ "height": h, "width": w, "id": imgid&#125;) f = open(os.path.join(label_path, file)) for line in f.readlines(): line = "".join(line).strip("\n").split(" ") # a bbox has 4 points, a category name and a difficulty if len(line) != 10: print(path, file) else: annid += 1 catid = util.wordname_15.index(line[-2]) + 1 w_bbox = int(line[4][:-2]) - int(line[0][:-2]) h_bbox = int(line[5][:-2]) - int(line[1][:-2]) bbox = [line[0], line[1], str(w_bbox)+'.0', str(h_bbox)+'.0'] annotations.append(&#123;"id": annid, "image_id": imgid, "category_id": catid, \ "segmentation": [line[0:8]], "area": float(w_bbox*h_bbox), \ "bbox": bbox, "iscrowd": 0&#125;) f.close()my_json = &#123;"info": info, "licenses": licenses, "images": images, "annotations": annotations, "categories": categories&#125;with open("/home/lxy/dota/data/coco/annotations/train.json", "w+") as f: json.dump(my_json, f) print("writing json file done!") 检查转换结果 这里需要用到 COCO API，具体用法参考 repo 里的 demo 文件，读取转换完成的数据集并显示标注结果，观察标注是否有误 完]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>数据集</tag>
        <tag>COCO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NP-Hard问题]]></title>
    <url>%2F2018%2F12%2F07%2FNP-Hard%2F</url>
    <content type="text"><![CDATA[简单理解 NP, P, NP-Complete 和 NP-Hard 参考：https://www.cnblogs.com/sancyun/p/4250360.html P 是一类可以通过确定性图灵机（以下简称 图灵机）在多项式时间 (Polynomial time) 内解决的问题集合。 NP 是一类可以通过非确定性图灵机 ( Non-deterministic Turing Machine) 在多项式时间 (Polynomial time) 内解决的决策问题集合。 P 是 NP 的子集，也就是说任何可以被图灵机在多项式时间内解决的问题都可以被非确定性的图灵机解决。 接下来说说 NP 里最难得问题 NP-Complete。 其定义如下， 如果一个决策问题 L 是 NP-Complete 的，那么 L 具备以下两个性质： L 是 NP（给定一个解决 NP-Complete 的方案 (solution，感兴趣的读者可以思考一下 solution 和 answer 的区别)，可以很快验证是否可行，但不存在已知高效的方案 。） NP 里的任何问题可以在多项式时间内转为 L。 而 NP-Hard 只需要具备 NP-Complete 的第二个性质，因此 NP-Complete 是 NP-Hard 的子集。 这四者的关系如下图（假设 P!= NP）：]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>NP-Hard</tag>
      </tags>
  </entry>
</search>
